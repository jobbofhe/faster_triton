#!/usr/bin/env python3

import torch
import sys
import os

# å°†å½“å‰ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from kernels.triton_kernels import (
    rms_norm,
    rotary_pos_emb,
    rotate_half,
    apply_rotary_pos_emb,
    get_attention_mask
)

# å®šä¹‰æµ‹è¯•è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# æµ‹è¯•å‚æ•°
batch_size = 4
seq_len = 128
hidden_size = 4096
dim = 128  # ç¡®ä¿dimæ˜¯2çš„å¹‚ï¼Œé€‚åˆä½œä¸ºç¼–è¯‘æ—¶å¸¸é‡

# æµ‹è¯•RMSNorm
def test_rms_norm():
    print("\n=== æµ‹è¯•RMSNorm ===")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    x = torch.randn(batch_size, seq_len, hidden_size, device=device, requires_grad=True)
    weight = torch.randn(hidden_size, device=device, requires_grad=True)
    epsilon = 1e-6
    
    # PyTorchå®ç°ï¼ˆç”¨äºæ¯”è¾ƒï¼‰
    def torch_rms_norm(x, weight, epsilon):
        variance = x.pow(2).mean(-1, keepdim=True)
        x_normalized = x * torch.rsqrt(variance + epsilon)
        return x_normalized * weight
    
    # è®¡ç®—ç»“æœ
    torch_result = torch_rms_norm(x, weight, epsilon)
    triton_result = rms_norm(x, weight, epsilon)
    
    # æ¯”è¾ƒç»“æœ
    print(f"PyTorchç»“æœå½¢çŠ¶: {torch_result.shape}")
    print(f"Tritonç»“æœå½¢çŠ¶: {triton_result.shape}")
    print(f"ç»“æœè¯¯å·®: {torch.max(torch.abs(torch_result - triton_result))}")
    print(f"ç»“æœæ˜¯å¦æ¥è¿‘: {torch.allclose(torch_result, triton_result, rtol=1e-5, atol=1e-5)}")
    
    return torch.allclose(torch_result, triton_result, rtol=1e-5, atol=1e-5)

# æµ‹è¯•RotaryPositionalEmbedding
def test_rotary_pos_emb():
    print("\n=== æµ‹è¯•RotaryPositionalEmbedding ===")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    max_seq_len = seq_len
    inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, device=device).float() / dim))
    
    # PyTorchå®ç°ï¼ˆç”¨äºæ¯”è¾ƒï¼‰
    def torch_rotary_pos_emb(inv_freq, max_seq_len):
        t = torch.arange(max_seq_len, device=inv_freq.device, dtype=torch.float32)
        freqs = torch.einsum("i,j->ij", t, inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        cos_emb = emb.cos()
        sin_emb = emb.sin()
        return cos_emb, sin_emb
    
    # è®¡ç®—ç»“æœ
    torch_cos, torch_sin = torch_rotary_pos_emb(inv_freq, max_seq_len)
    triton_cos, triton_sin = rotary_pos_emb(inv_freq, max_seq_len)
    
    # æ¯”è¾ƒç»“æœ
    print(f"PyTorch coså½¢çŠ¶: {torch_cos.shape}, sinå½¢çŠ¶: {torch_sin.shape}")
    print(f"Triton coså½¢çŠ¶: {triton_cos.shape}, sinå½¢çŠ¶: {triton_sin.shape}")
    print(f"cosè¯¯å·®: {torch.max(torch.abs(torch_cos - triton_cos))}")
    print(f"sinè¯¯å·®: {torch.max(torch.abs(torch_sin - triton_sin))}")
    print(f"cosæ˜¯å¦æ¥è¿‘: {torch.allclose(torch_cos, triton_cos, rtol=1e-5, atol=1e-5)}")
    print(f"sinæ˜¯å¦æ¥è¿‘: {torch.allclose(torch_sin, triton_sin, rtol=1e-5, atol=1e-5)}")
    
    return (torch.allclose(torch_cos, triton_cos, rtol=1e-5, atol=1e-5) and 
            torch.allclose(torch_sin, triton_sin, rtol=1e-5, atol=1e-5))

# æµ‹è¯•rotate_half
def test_rotate_half():
    print("\n=== æµ‹è¯•rotate_half ===")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    x = torch.randn(batch_size, seq_len, dim, device=device, requires_grad=True)
    
    # PyTorchå®ç°ï¼ˆç”¨äºæ¯”è¾ƒï¼‰
    def torch_rotate_half(x):
        x1 = x[..., :x.shape[-1] // 2]
        x2 = x[..., x.shape[-1] // 2:]
        return torch.cat((-x2, x1), dim=-1)
    
    # è®¡ç®—ç»“æœ
    torch_result = torch_rotate_half(x)
    triton_result = rotate_half(x)
    
    # æ¯”è¾ƒç»“æœ
    print(f"PyTorchç»“æœå½¢çŠ¶: {torch_result.shape}")
    print(f"Tritonç»“æœå½¢çŠ¶: {triton_result.shape}")
    print(f"ç»“æœè¯¯å·®: {torch.max(torch.abs(torch_result - triton_result))}")
    print(f"ç»“æœæ˜¯å¦æ¥è¿‘: {torch.allclose(torch_result, triton_result, rtol=1e-5, atol=1e-5)}")
    
    return torch.allclose(torch_result, triton_result, rtol=1e-5, atol=1e-5)

# æµ‹è¯•apply_rotary_pos_emb
def test_apply_rotary_pos_emb():
    print("\n=== æµ‹è¯•apply_rotary_pos_emb ===")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    q = torch.randn(batch_size, seq_len, dim, device=device, requires_grad=True)
    k = torch.randn(batch_size, seq_len, dim, device=device, requires_grad=True)
    
    # åˆ›å»ºsinå’ŒcosåµŒå…¥
    inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, device=device).float() / dim))
    t = torch.arange(seq_len, device=device, dtype=torch.float32)
    freqs = torch.einsum("i,j->ij", t, inv_freq)
    emb = torch.cat((freqs, freqs), dim=-1)
    cos_emb = emb.cos()
    sin_emb = emb.sin()
    
    # PyTorchå®ç°ï¼ˆç”¨äºæ¯”è¾ƒï¼‰
    def torch_apply_rotary_pos_emb(q, k, cos_emb, sin_emb):
        def rotate_half(x):
            x1 = x[..., :x.shape[-1] // 2]
            x2 = x[..., x.shape[-1] // 2:]
            return torch.cat((-x2, x1), dim=-1)
        
        q_rot = q * cos_emb + rotate_half(q) * sin_emb
        k_rot = k * cos_emb + rotate_half(k) * sin_emb
        return q_rot, k_rot
    
    # è®¡ç®—ç»“æœ
    torch_q, torch_k = torch_apply_rotary_pos_emb(q, k, cos_emb, sin_emb)
    triton_q, triton_k = apply_rotary_pos_emb(q, k, cos_emb, sin_emb)
    
    # æ¯”è¾ƒç»“æœ
    print(f"PyTorch qå½¢çŠ¶: {torch_q.shape}, kå½¢çŠ¶: {torch_k.shape}")
    print(f"Triton qå½¢çŠ¶: {triton_q.shape}, kå½¢çŠ¶: {triton_k.shape}")
    print(f"qè¯¯å·®: {torch.max(torch.abs(torch_q - triton_q))}")
    print(f"kè¯¯å·®: {torch.max(torch.abs(torch_k - triton_k))}")
    print(f"qæ˜¯å¦æ¥è¿‘: {torch.allclose(torch_q, triton_q, rtol=1e-5, atol=1e-5)}")
    print(f"kæ˜¯å¦æ¥è¿‘: {torch.allclose(torch_k, triton_k, rtol=1e-5, atol=1e-5)}")
    
    return (torch.allclose(torch_q, triton_q, rtol=1e-5, atol=1e-5) and 
            torch.allclose(torch_k, triton_k, rtol=1e-5, atol=1e-5))

# æµ‹è¯•get_attention_mask
def test_get_attention_mask():
    print("\n=== æµ‹è¯•get_attention_mask ===")
    
    # PyTorchå®ç°ï¼ˆç”¨äºæ¯”è¾ƒï¼‰
    def torch_get_attention_mask(seq_len):
        mask = torch.full((seq_len, seq_len), -float('inf'), device=device)
        mask = torch.triu(mask, diagonal=1)
        return mask.unsqueeze(0).unsqueeze(0)
    
    # è®¡ç®—ç»“æœ
    torch_result = torch_get_attention_mask(seq_len)
    triton_result = get_attention_mask(seq_len, device)
    
    # æ¯”è¾ƒç»“æœ
    print(f"PyTorchç»“æœå½¢çŠ¶: {torch_result.shape}")
    print(f"Tritonç»“æœå½¢çŠ¶: {triton_result.shape}")
    print(f"ç»“æœè¯¯å·®: {torch.max(torch.abs(torch_result - triton_result))}")
    print(f"ç»“æœæ˜¯å¦æ¥è¿‘: {torch.allclose(torch_result, triton_result, rtol=1e-5, atol=1e-5)}")
    
    return torch.allclose(torch_result, triton_result, rtol=1e-5, atol=1e-5)

# è¿è¡Œæ‰€æœ‰æµ‹è¯•
def run_all_tests():
    print("å¼€å§‹æµ‹è¯•æ‰€æœ‰Tritoné£æ ¼ç®—å­...")
    
    tests = [
        test_rms_norm,
        test_rotary_pos_emb,
        test_rotate_half,
        test_apply_rotary_pos_emb,
        test_get_attention_mask
    ]
    
    results = []
    for test in tests:
        try:
            result = test()
            results.append(result)
        except Exception as e:
            print(f"æµ‹è¯• {test.__name__} å¤±è´¥: {e}")
            results.append(False)
    
    print("\n=== æµ‹è¯•æ€»ç»“ ===")
    for i, (test, result) in enumerate(zip(tests, results), 1):
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"æµ‹è¯• {i}: {test.__name__} - {status}")
    
    if all(results):
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼")
        return 1

if __name__ == "__main__":
    exit(run_all_tests())
