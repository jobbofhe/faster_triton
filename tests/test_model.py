#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Llama3-8B æ¨¡å‹æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯æ¨¡å‹å®šä¹‰å’Œ Triton å†…æ ¸æ˜¯å¦æ­£ç¡®é›†æˆ
"""

import os
import sys
import torch
import json
from models.llama3_model import Llama3ForCausalLM
from configs.llama3_8b_config import LLAMA3_8B_CONFIG

def test_model_initialization():
    """æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–"""
    print("\n=== æµ‹è¯•æ¨¡å‹åˆå§‹åŒ– ===")
    
    try:
        # åˆå§‹åŒ–æ¨¡å‹
        model = Llama3ForCausalLM(LLAMA3_8B_CONFIG)
        
        # æ£€æŸ¥æ¨¡å‹å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        print(f"   æ€»å‚æ•°æ•°é‡: {total_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        return model
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        raise

def test_forward_pass(model):
    """æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­"""
    print("\n=== æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­ ===")
    
    try:
        # åˆ›å»ºéšæœºè¾“å…¥
        batch_size = 2
        seq_len = 128
        vocab_size = model.config.vocab_size
        
        input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
        labels = torch.randint(0, vocab_size, (batch_size, seq_len))
        
        print(f"è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
        print(f"æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
        
        # å‰å‘ä¼ æ’­
        loss, logits = model(input_ids, labels=labels)
        
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"   æŸå¤±å€¼: {loss.item():.4f}")
        print(f"   Logits å½¢çŠ¶: {logits.shape}")
        
        return True
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_gpu_usage(model):
    """æµ‹è¯• GPU åŠ é€Ÿ"""
    print("\n=== æµ‹è¯• GPU åŠ é€Ÿ ===")
    
    try:
        if torch.cuda.is_available():
            device = torch.device("cuda")
            model = model.to(device)
            
            # åˆ›å»º GPU è¾“å…¥
            batch_size = 2
            seq_len = 128
            vocab_size = model.config.vocab_size
            
            input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)
            labels = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)
            
            # å‰å‘ä¼ æ’­
            loss, logits = model(input_ids, labels=labels)
            
            print(f"âœ… GPU åŠ é€Ÿæµ‹è¯•æˆåŠŸ")
            print(f"   GPU è®¾å¤‡: {torch.cuda.get_device_name(0)}")
            print(f"   æŸå¤±å€¼: {loss.item():.4f}")
            
            return True
        else:
            print(f"âš ï¸  æœªæ£€æµ‹åˆ° GPUï¼Œè·³è¿‡ GPU åŠ é€Ÿæµ‹è¯•")
            return False
    except Exception as e:
        print(f"âŒ GPU åŠ é€Ÿæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_triton_kernels():
    """æµ‹è¯• Triton å†…æ ¸æ˜¯å¦æ­£ç¡®å¯¼å…¥"""
    print("\n=== æµ‹è¯• Triton å†…æ ¸å¯¼å…¥ ===")
    
    try:
        from kernels.triton_kernels import fused_attention, fused_mlp, rms_norm
        
        print(f"âœ… Triton å†…æ ¸å¯¼å…¥æˆåŠŸ")
        print(f"   å¯ç”¨å†…æ ¸: fused_attention, fused_mlp, rms_norm")
        
        return True
    except Exception as e:
        print(f"âŒ Triton å†…æ ¸å¯¼å…¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸš€ Llama3-8B æ¨¡å‹æµ‹è¯•")
    print("=" * 50)
    
    try:
        # è®¾ç½®è®¾å¤‡
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # è¿è¡Œæµ‹è¯•
        results = []
        
        # æµ‹è¯• 1: Triton å†…æ ¸å¯¼å…¥
        results.append(test_triton_kernels())
        
        # æµ‹è¯• 2: æ¨¡å‹åˆå§‹åŒ–
        model = test_model_initialization()
        results.append(True)
        
        # æµ‹è¯• 3: å‰å‘ä¼ æ’­
        results.append(test_forward_pass(model))
        
        # æµ‹è¯• 4: GPU åŠ é€Ÿ
        results.append(test_gpu_usage(model))
        
        # æ±‡æ€»ç»“æœ
        print("\n" + "=" * 50)
        print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
        print(f"æ€»æµ‹è¯•æ•°: {len(results)}")
        print(f"é€šè¿‡æµ‹è¯•: {sum(results)}")
        print(f"å¤±è´¥æµ‹è¯•: {len(results) - sum(results)}")
        
        if all(results):
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹å¯ä»¥æ­£å¸¸å·¥ä½œã€‚")
            sys.exit(0)
        else:
            print("ğŸ’¥ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œæ¨¡å‹éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)